python3 run_pretraining.py \
 --input_file="gs://arabert_data/ar_gpt2/pretraining_data/*" \
 --output_dir="gs://arabert_data/ar_gpt2/xlarge_pretraining_model_simple/" \
 --config_file="config/xl_hparams.json" \
 --batch_size=128 \
 --eval_batch_size=128 \
 --num_train_steps=400000 \
 --num_warmup_steps=10000 \
 --learning_rate=1e-4 \
 --save_checkpoints_steps=5000 \
 --max_seq_length=1024 \
 --max_eval_steps=10 \
 --optimizer="adafactor" \
 --iterations_per_loop=5000 \
 --keep_checkpoint_max=10 \
 --use_tpu=True \
 --tpu_name="arabert128" \
 --num_tpu_cores=128 \
 --do_train=True \
 --do_eval=False \
 --use_memory_saving_gradients=False